"""
OpenAI å…¼å®¹çš„ Chat Completions API

æä¾›å®Œå…¨å…¼å®¹ OpenAI æ ¼å¼çš„ /v1/chat/completions ç«¯ç‚¹ã€‚
"""

import logging
import time
import uuid
from typing import AsyncGenerator

from fastapi import APIRouter, Depends
from fastapi.responses import StreamingResponse
from langchain_core.messages import AIMessage, HumanMessage

from src.agent.main.graph import get_agent_app
from src.core.config import settings
from src.core.security import verify_api_key
from src.models.openai_schema import (
    ChatCompletionChoice,
    ChatCompletionChunk,
    ChatCompletionChunkChoice,
    ChatCompletionChunkDelta,
    ChatCompletionRequest,
    ChatCompletionResponse,
    ChatCompletionUsage,
    ChatMessage,
    OpenAIModelList,
    OpenAIModelRef,
)


def _validate_message_source(message: str) -> bool:
    """
    éªŒè¯æ¶ˆæ¯æ¥æºï¼Œè¿‡æ»¤éç”¨æˆ·æ¥æºçš„æ¶ˆæ¯

    Args:
        message: å¾…éªŒè¯çš„æ¶ˆæ¯å†…å®¹

    Returns:
        bool: Trueè¡¨ç¤ºæ˜¯ç”¨æˆ·æ¥æºçš„æ¶ˆæ¯ï¼ŒFalseè¡¨ç¤ºåº”è¯¥è¢«è¿‡æ»¤
    """
    # æ£€æŸ¥æ˜¯å¦å¯ç”¨æ¶ˆæ¯è¿‡æ»¤
    if not settings.message_filter_enabled:
        return True

    # æ£€æŸ¥æ˜¯å¦åŒ…å«ç³»ç»Ÿæ ‡è¯†ç¬¦
    system_indicators = [
        "system:", "assistant:", "ai:", "bot:", "agent:",
        "SYSTEM:", "ASSISTANT:", "AI:", "BOT:", "AGENT:"
    ]

    message_lower = message.lower()
    for indicator in system_indicators:
        if indicator.lower() in message_lower:
            logger.warning(f"System message detected: {indicator}")
            return False

    # æ£€æŸ¥æ˜¯å¦ä»¥ç³»ç»Ÿæ ‡è¯†ç¬¦å¼€å¤´
    if message.strip().startswith(tuple(system_indicators)):
        logger.warning("Message starts with system indicator")
        return False

    # æ£€æŸ¥æ˜¯å¦åŒ…å«è¿‡å¤šçš„æŠ€æœ¯æœ¯è¯­ï¼ˆå¯èƒ½æ˜¯ç³»ç»Ÿæ¶ˆæ¯ï¼‰
    technical_terms = [term.strip() for term in settings.technical_terms.split(",") if term.strip()]
    technical_count = sum(1 for term in technical_terms if term.lower() in message_lower)
    if technical_count >= settings.technical_terms_threshold:
        logger.warning(f"Too many technical terms detected: {technical_count} (threshold: {settings.technical_terms_threshold})")
        return False

    return True

logger = logging.getLogger(__name__)

router = APIRouter(dependencies=[Depends(verify_api_key)])


@router.get("/models")
async def list_models() -> OpenAIModelList:
    """
    OpenAI å…¼å®¹çš„æ¨¡å‹åˆ—è¡¨ç«¯ç‚¹ (/v1/models)ã€‚

    æ”¯æŒæ··åˆæ¨¡å‹ç»„åˆï¼š
    - LLM å’Œ Embedding å¯ä»¥æ¥è‡ªä¸åŒæä¾›å•†
    - æ”¯æŒæ¨¡å‹åˆ«ååŠŸèƒ½
    - æ”¯æŒç¡…åŸºæµåŠ¨å¹³å°ç­‰æ–°æä¾›å•†

    âš ï¸ å½“å¯ç”¨æ¨¡å‹åˆ«ååŠŸèƒ½æ—¶ï¼ˆMODEL_ALIAS_ENABLED=trueï¼‰ï¼š
    - è¿”å›é…ç½®çš„åˆ«åæ¨¡å‹ï¼ˆå¦‚ gpt-4o-miniï¼‰
    - owned_by å­—æ®µä¸ºé…ç½®çš„å€¼ï¼ˆå¦‚ openaiï¼‰
    - ä»…è¿”å›èŠå¤©æ¨¡å‹ï¼ˆä¸è¿”å› embedding æ¨¡å‹ï¼‰

    å½“ç¦ç”¨æ—¶ï¼ˆé»˜è®¤ï¼‰ï¼š
    - è¿”å›å®é™…æ¨¡å‹åç§°ï¼ˆå¦‚ deepseek-chatï¼‰
    - owned_by æ˜¾ç¤ºå®é™…æä¾›å•†ï¼ˆå¦‚ provider:deepseekï¼‰
    """
    now_ts = int(time.time())
    id_to_ref: dict[str, OpenAIModelRef] = {}

    # åˆ¤æ–­æ˜¯å¦å¯ç”¨åˆ«å
    if settings.model_alias_enabled:
        # ä½¿ç”¨åˆ«åæ¨¡å‹
        logger.info(
            f"ğŸ­ Model alias enabled: returning alias '{settings.model_alias_name}' "
            f"(actual: {settings.llm_model_name})"
        )
        id_to_ref[settings.model_alias_name] = OpenAIModelRef(
            id=settings.model_alias_name,
            created=now_ts,
            owned_by=settings.model_alias_owned_by,
        )

        # å¦‚æœé…ç½®ä¸ºä¸éšè— embedding æ¨¡å‹ï¼Œæ·»åŠ å®ƒ
        if not settings.hide_embedding_models:
            try:
                embedding_id = settings.embedding_model_name
                if embedding_id and embedding_id not in id_to_ref:
                    id_to_ref[embedding_id] = OpenAIModelRef(
                        id=embedding_id,
                        created=now_ts,
                        owned_by=f"provider:{settings.embedding_provider}",
                    )
            except Exception:
                pass
    else:
        # è¿”å›å®é™…æ¨¡å‹åï¼ˆæ”¯æŒæ··åˆæ¨¡å‹ç»„åˆï¼‰
        chat_model_id = settings.llm_model_name
        id_to_ref[chat_model_id] = OpenAIModelRef(
            id=chat_model_id,
            created=now_ts,
            owned_by=f"provider:{settings.llm_provider}",
        )

        # æ·»åŠ  Embedding æ¨¡å‹ï¼ˆå¦‚æœé…ç½®æ˜¾ç¤ºï¼‰
        if not settings.hide_embedding_models:
            try:
                embedding_id = settings.embedding_model_name
                if embedding_id and embedding_id not in id_to_ref:
                    id_to_ref[embedding_id] = OpenAIModelRef(
                        id=embedding_id,
                        created=now_ts,
                        owned_by=f"provider:{settings.embedding_provider}",
                    )
            except Exception:
                pass

    return OpenAIModelList(data=list(id_to_ref.values()))

@router.post("/chat/completions", response_model=None)
async def chat_completions(
    request: ChatCompletionRequest,
) -> ChatCompletionResponse | StreamingResponse:
    """
    OpenAI å…¼å®¹çš„ Chat Completions ç«¯ç‚¹

    æ”¯æŒæµå¼å’Œéæµå¼å“åº”ã€‚
    """
    logger.info(
        f"ğŸ“¨ Received chat completion request: "
        f"messages={len(request.messages)}, stream={request.stream}"
    )

    # æ¨¡å‹åˆ«åæ˜ å°„ï¼ˆæ”¯æŒæ¥å—åˆ«åè¯·æ±‚ï¼‰
    actual_model = settings.llm_model_name  # å®é™…ä½¿ç”¨çš„æ¨¡å‹
    requested_model = request.model  # ç”¨æˆ·è¯·æ±‚çš„æ¨¡å‹

    if settings.model_alias_enabled:
        if requested_model == settings.model_alias_name:
            logger.info(
                f"ğŸ­ Model alias mapping: request='{requested_model}' â†’ actual='{actual_model}'"
            )
        else:
            logger.warning(
                f"âš ï¸ Unexpected model requested: '{requested_model}' "
                f"(expected alias: '{settings.model_alias_name}'). "
                f"Still using actual model: '{actual_model}'"
            )
    else:
        # åˆ«åæœªå¯ç”¨ï¼Œç›´æ¥ä½¿ç”¨è¯·æ±‚çš„æ¨¡å‹åï¼ˆä½†å®é™…ä»ç”¨é…ç½®çš„æ¨¡å‹ï¼‰
        logger.debug(f"Model requested: '{requested_model}', actual: '{actual_model}'")

    # ç”Ÿæˆå”¯ä¸€ ID
    completion_id = f"chatcmpl-{uuid.uuid4().hex[:8]}"
    created_timestamp = int(time.time())

    # æå–ç”¨æˆ·æ¶ˆæ¯ï¼ˆæœ€åä¸€æ¡ï¼‰
    last_message = request.messages[-1]
    if last_message.role != "user":
        # å¦‚æœæœ€åä¸€æ¡ä¸æ˜¯ç”¨æˆ·æ¶ˆæ¯ï¼Œä½¿ç”¨å€’æ•°ç¬¬äºŒæ¡
        user_messages = [msg for msg in request.messages if msg.role == "user"]
        if not user_messages:
            # æ²¡æœ‰ç”¨æˆ·æ¶ˆæ¯ï¼Œè¿”å›é”™è¯¯
            from fastapi import HTTPException

            raise HTTPException(
                status_code=400,
                detail={
                    "error": {
                        "message": "No user message found in request",
                        "type": "invalid_request_error",
                        "code": "no_user_message",
                    }
                },
            )
        user_message = user_messages[-1].content
    else:
        user_message = last_message.content

    # ç”Ÿæˆ session_idï¼ˆå¯é€‰ï¼šä»è¯·æ±‚ä¸­æå–ï¼‰
    session_id = f"session-{uuid.uuid4().hex[:12]}"

    # æµå¼å“åº”
    if request.stream:
        return StreamingResponse(
            _stream_response(
                user_message=user_message,
                session_id=session_id,
                completion_id=completion_id,
                created_timestamp=created_timestamp,
                model=request.model,
                requested_model=requested_model,
            ),
            media_type="text/event-stream",
        )

    # éæµå¼å“åº”
    return await _non_stream_response(
        user_message=user_message,
        session_id=session_id,
        completion_id=completion_id,
        created_timestamp=created_timestamp,
        model=request.model,
        requested_model=requested_model,
    )


async def _non_stream_response(
    user_message: str,
    session_id: str,
    completion_id: str,
    created_timestamp: int,
    model: str,
    requested_model: str,
) -> ChatCompletionResponse:
    """éæµå¼å“åº”"""
    from src.agent.main.nodes import _get_filter_reason, _is_valid_user_query

    # åœ¨éæµå¼è·¯å¾„ä¸­ä¹Ÿè¿›è¡Œæ¶ˆæ¯éªŒè¯ï¼Œè¿‡æ»¤å¤–éƒ¨æŒ‡ä»¤æ¨¡æ¿
    if not _validate_message_source(user_message):
        logger.warning("âš ï¸ éæµå¼APIå±‚è¿‡æ»¤éç”¨æˆ·æ¥æºæ¶ˆæ¯")
        return ChatCompletionResponse(
            id=completion_id,
            created=created_timestamp,
            model=requested_model,
            choices=[
                ChatCompletionChoice(
                    index=0,
                    message=ChatMessage(
                        role="assistant",
                        content="æŠ±æ­‰ï¼Œç³»ç»Ÿæ¶ˆæ¯æ— æ³•å¤„ç†ã€‚è¯·å‘é€ç”¨æˆ·é—®é¢˜ã€‚"
                    ),
                    finish_reason="content_filter",
                )
            ],
            usage=ChatCompletionUsage(
                prompt_tokens=0, completion_tokens=0, total_tokens=0
            ),
        )

    if not _is_valid_user_query(user_message):
        filter_reason = _get_filter_reason(user_message)
        logger.warning(f"âš ï¸ éæµå¼APIå±‚è¿‡æ»¤æ— æ•ˆæ¶ˆæ¯ (reason: {filter_reason}, length: {len(user_message)})")
        return ChatCompletionResponse(
            id=completion_id,
            created=created_timestamp,
            model=requested_model,
            choices=[
                ChatCompletionChoice(
                    index=0,
                    message=ChatMessage(
                        role="assistant",
                        content="æŠ±æ­‰ï¼Œæ‚¨çš„æ¶ˆæ¯åŒ…å«æ— æ•ˆå†…å®¹ï¼Œæ— æ³•å¤„ç†ã€‚è¯·é‡æ–°å‘é€æ‚¨çš„é—®é¢˜ã€‚"
                    ),
                    finish_reason="content_filter",
                )
            ],
            usage=ChatCompletionUsage(
                prompt_tokens=0, completion_tokens=0, total_tokens=0
            ),
        )

    # è°ƒç”¨ Agent
    app = get_agent_app()

    initial_state = {
        "messages": [HumanMessage(content=user_message)],
        "retrieved_docs": [],
        "tool_calls": [],
        "session_id": session_id,
        "next_step": None,
        "error": None,
        "confidence_score": None,
    }

    config = {"configurable": {"thread_id": session_id}}

    try:
        result = await app.ainvoke(initial_state, config)

        # æå– AI å“åº”
        ai_message = result["messages"][-1]
        if isinstance(ai_message, AIMessage):
            response_content = ai_message.content
        else:
            response_content = str(ai_message)

        # è®¡ç®— Token ä½¿ç”¨ï¼ˆç®€åŒ–ç‰ˆï¼Œä½¿ç”¨å­—ç¬¦æ•°ä¼°ç®—ï¼‰
        prompt_tokens = len(user_message) // 4
        completion_tokens = len(response_content) // 4
        total_tokens = prompt_tokens + completion_tokens

        # æ„å»º OpenAI æ ¼å¼å“åº”
        return ChatCompletionResponse(
            id=completion_id,
            created=created_timestamp,
            model=requested_model,  # è¿”å›ç”¨æˆ·è¯·æ±‚çš„æ¨¡å‹åï¼ˆä¿æŒä¸€è‡´æ€§ï¼‰
            choices=[
                ChatCompletionChoice(
                    index=0,
                    message=ChatMessage(role="assistant", content=response_content),
                    finish_reason="stop",
                )
            ],
            usage=ChatCompletionUsage(
                prompt_tokens=prompt_tokens,
                completion_tokens=completion_tokens,
                total_tokens=total_tokens,
            ),
        )

    except Exception as e:
        logger.error(f"âŒ Agent execution failed: {e}")
        # è¿”å›é”™è¯¯å“åº”
        return ChatCompletionResponse(
            id=completion_id,
            created=created_timestamp,
            model=requested_model,  # è¿”å›ç”¨æˆ·è¯·æ±‚çš„æ¨¡å‹åï¼ˆä¿æŒä¸€è‡´æ€§ï¼‰
            choices=[
                ChatCompletionChoice(
                    index=0,
                    message=ChatMessage(
                        role="assistant", content="æŠ±æ­‰ï¼Œç³»ç»Ÿé‡åˆ°äº†é—®é¢˜ï¼Œè¯·ç¨åå†è¯•ã€‚"
                    ),
                    finish_reason="stop",
                )
            ],
            usage=ChatCompletionUsage(
                prompt_tokens=0, completion_tokens=0, total_tokens=0
            ),
        )


async def _stream_response(
    user_message: str,
    session_id: str,
    completion_id: str,
    created_timestamp: int,
    model: str,
    requested_model: str,
) -> AsyncGenerator[str, None]:
    """æµå¼å“åº”ï¼ˆSSEï¼‰"""
    from src.agent.main.nodes import _is_valid_user_query

    app = get_agent_app()

    # åœ¨APIå±‚è¿›è¡Œæ¶ˆæ¯æ¥æºéªŒè¯
    # æ£€æŸ¥æ¶ˆæ¯æ¥æºï¼Œè¿‡æ»¤éç”¨æˆ·æ¥æºçš„æ¶ˆæ¯
    if not _validate_message_source(user_message):
        logger.warning("âš ï¸ APIå±‚è¿‡æ»¤éç”¨æˆ·æ¥æºæ¶ˆæ¯")
        # è¿”å›é”™è¯¯å“åº”ï¼Œä¸è¿›å…¥Agentæµç¨‹
        error_chunk = ChatCompletionChunk(
            id=completion_id,
            created=created_timestamp,
            model=requested_model,
            choices=[
                ChatCompletionChunkChoice(
                    index=0,
                    delta=ChatCompletionChunkDelta(
                        content="æŠ±æ­‰ï¼Œç³»ç»Ÿæ¶ˆæ¯æ— æ³•å¤„ç†ã€‚è¯·å‘é€ç”¨æˆ·é—®é¢˜ã€‚"
                    ),
                    finish_reason="content_filter",
                )
            ],
        )
        yield f"data: {error_chunk.model_dump_json()}\n\n"
        yield "data: [DONE]\n\n"
        return

    # åœ¨APIå±‚è¿›è¡Œæ¶ˆæ¯éªŒè¯ï¼Œè¿‡æ»¤å¤–éƒ¨æŒ‡ä»¤æ¨¡æ¿
    if not _is_valid_user_query(user_message):
        from src.agent.main.nodes import _get_filter_reason
        filter_reason = _get_filter_reason(user_message)
        logger.warning(f"âš ï¸ APIå±‚è¿‡æ»¤æ— æ•ˆæ¶ˆæ¯ (reason: {filter_reason}, length: {len(user_message)})")
        # è¿”å›é”™è¯¯å“åº”ï¼Œä¸è¿›å…¥Agentæµç¨‹
        error_chunk = ChatCompletionChunk(
            id=completion_id,
            created=created_timestamp,
            model=requested_model,
            choices=[
                ChatCompletionChunkChoice(
                    index=0,
                    delta=ChatCompletionChunkDelta(
                        content="æŠ±æ­‰ï¼Œæ‚¨çš„æ¶ˆæ¯åŒ…å«æ— æ•ˆå†…å®¹ï¼Œæ— æ³•å¤„ç†ã€‚è¯·é‡æ–°å‘é€æ‚¨çš„é—®é¢˜ã€‚"
                    ),
                    finish_reason="content_filter",
                )
            ],
        )
        yield f"data: {error_chunk.model_dump_json()}\n\n"
        yield "data: [DONE]\n\n"
        return

    initial_state = {
        "messages": [HumanMessage(content=user_message)],
        "retrieved_docs": [],
        "tool_calls": [],
        "session_id": session_id,
        "next_step": None,
        "error": None,
        "confidence_score": None,
    }

    config = {"configurable": {"thread_id": session_id}}

    try:
        # å‘é€åˆå§‹ chunkï¼ˆroleï¼‰
        first_chunk = ChatCompletionChunk(
            id=completion_id,
            created=created_timestamp,
            model=requested_model,  # è¿”å›ç”¨æˆ·è¯·æ±‚çš„æ¨¡å‹åï¼ˆä¿æŒä¸€è‡´æ€§ï¼‰
            choices=[
                ChatCompletionChunkChoice(
                    index=0,
                    delta=ChatCompletionChunkDelta(role="assistant"),
                    finish_reason=None,
                )
            ],
        )
        yield f"data: {first_chunk.model_dump_json()}\n\n"

        # å¯¼å…¥AIMessageç±»åˆ°å‡½æ•°ä½œç”¨åŸŸ
        from langchain_core.messages import AIMessage

        # æµå¼æ‰§è¡Œ Agent
        async for chunk in app.astream(initial_state, config):
            # æ£€æŸ¥æ˜¯å¦æœ‰æ–°çš„ AI æ¶ˆæ¯
            if "llm" in chunk:  # LLM èŠ‚ç‚¹çš„è¾“å‡º
                llm_output = chunk["llm"]

                # ç±»å‹æ£€æŸ¥ï¼šç¡®ä¿llm_outputæ˜¯å­—å…¸ç±»å‹
                if isinstance(llm_output, dict):
                    messages = llm_output.get("messages", [])
                elif isinstance(llm_output, str):
                    # å¦‚æœllm_outputæ˜¯å­—ç¬¦ä¸²ï¼Œå¯èƒ½æ˜¯é”™è¯¯ä¿¡æ¯æˆ–ç›´æ¥å†…å®¹
                    logger.warning(f"âš ï¸ LLM output is string: {llm_output}")
                    # åˆ›å»ºä¸€ä¸ªä¸´æ—¶çš„AIæ¶ˆæ¯
                    messages = [AIMessage(content=llm_output)]
                else:
                    logger.error(f"âŒ Unexpected llm_output type: {type(llm_output)}")
                    continue

                if messages:
                    ai_message = messages[-1]
                    if isinstance(ai_message, AIMessage):
                        content = ai_message.content

                        # å‘é€å†…å®¹ chunk
                        # æ³¨æ„ï¼šè¿™é‡Œå‘é€å®Œæ•´å†…å®¹ï¼Œå®é™…åº”è¯¥å‘é€å¢é‡
                        # ä¸ºäº†ç®€åŒ–ï¼Œæˆ‘ä»¬ä¸€æ¬¡æ€§å‘é€ï¼ˆLangGraph ä¸åŸç”Ÿæ”¯æŒ token-by-token æµå¼ï¼‰
                        content_chunk = ChatCompletionChunk(
                            id=completion_id,
                            created=created_timestamp,
                            model=requested_model,  # è¿”å›ç”¨æˆ·è¯·æ±‚çš„æ¨¡å‹åï¼ˆä¿æŒä¸€è‡´æ€§ï¼‰
                            choices=[
                                ChatCompletionChunkChoice(
                                    index=0,
                                    delta=ChatCompletionChunkDelta(content=content),
                                    finish_reason=None,
                                )
                            ],
                        )
                        yield f"data: {content_chunk.model_dump_json()}\n\n"

        # å‘é€ç»“æŸ chunk
        final_chunk = ChatCompletionChunk(
            id=completion_id,
            created=created_timestamp,
            model=requested_model,  # è¿”å›ç”¨æˆ·è¯·æ±‚çš„æ¨¡å‹åï¼ˆä¿æŒä¸€è‡´æ€§ï¼‰
            choices=[
                ChatCompletionChunkChoice(
                    index=0,
                    delta=ChatCompletionChunkDelta(),
                    finish_reason="stop",
                )
            ],
        )
        yield f"data: {final_chunk.model_dump_json()}\n\n"

        # å‘é€ [DONE]
        yield "data: [DONE]\n\n"

    except Exception as e:
        logger.error(f"âŒ Streaming failed: {e}")
        # å‘é€é”™è¯¯ chunk
        error_chunk = ChatCompletionChunk(
            id=completion_id,
            created=created_timestamp,
            model=requested_model,  # è¿”å›ç”¨æˆ·è¯·æ±‚çš„æ¨¡å‹åï¼ˆä¿æŒä¸€è‡´æ€§ï¼‰
            choices=[
                ChatCompletionChunkChoice(
                    index=0,
                    delta=ChatCompletionChunkDelta(content="ç³»ç»Ÿé”™è¯¯ï¼Œè¯·ç¨åå†è¯•ã€‚"),
                    finish_reason="stop",
                )
            ],
        )
        yield f"data: {error_chunk.model_dump_json()}\n\n"
        yield "data: [DONE]\n\n"

