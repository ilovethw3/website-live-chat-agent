"""
OpenAI å…¼å®¹çš„ Chat Completions API

æä¾›å®Œå…¨å…¼å®¹ OpenAI æ ¼å¼çš„ /v1/chat/completions ç«¯ç‚¹ã€‚
"""

import logging
import time
import uuid
from typing import AsyncGenerator

from fastapi import APIRouter, Depends
from fastapi.responses import StreamingResponse
from langchain_core.messages import AIMessage, HumanMessage

from src.agent.graph import get_agent_app
from src.core.config import settings
from src.core.security import verify_api_key
from src.models.openai_schema import (
    ChatCompletionChoice,
    ChatCompletionChunk,
    ChatCompletionChunkChoice,
    ChatCompletionChunkDelta,
    ChatCompletionRequest,
    ChatCompletionResponse,
    ChatCompletionUsage,
    ChatMessage,
    OpenAIModelList,
    OpenAIModelRef,
)

logger = logging.getLogger(__name__)

router = APIRouter(dependencies=[Depends(verify_api_key)])


@router.get("/models")
async def list_models() -> OpenAIModelList:
    """OpenAI å…¼å®¹çš„æ¨¡å‹åˆ—è¡¨ç«¯ç‚¹ (/v1/models)ã€‚"""
    now_ts = int(time.time())

    id_to_ref: dict[str, OpenAIModelRef] = {}

    chat_model_id = settings.llm_model_name
    id_to_ref[chat_model_id] = OpenAIModelRef(
        id=chat_model_id,
        created=now_ts,
        owned_by=f"provider:{settings.llm_provider}",
    )

    try:
        embedding_id = settings.embedding_model
        if embedding_id and embedding_id not in id_to_ref:
            id_to_ref[embedding_id] = OpenAIModelRef(
                id=embedding_id,
                created=now_ts,
                owned_by=f"provider:{settings.embedding_provider}",
            )
    except Exception:
        pass

    return OpenAIModelList(data=list(id_to_ref.values()))

@router.post("/chat/completions", response_model=None)
async def chat_completions(
    request: ChatCompletionRequest,
) -> ChatCompletionResponse | StreamingResponse:
    """
    OpenAI å…¼å®¹çš„ Chat Completions ç«¯ç‚¹

    æ”¯æŒæµå¼å’Œéæµå¼å“åº”ã€‚
    """
    logger.info(
        f"ğŸ“¨ Received chat completion request: "
        f"messages={len(request.messages)}, stream={request.stream}"
    )

    # ç”Ÿæˆå”¯ä¸€ ID
    completion_id = f"chatcmpl-{uuid.uuid4().hex[:8]}"
    created_timestamp = int(time.time())

    # æå–ç”¨æˆ·æ¶ˆæ¯ï¼ˆæœ€åä¸€æ¡ï¼‰
    last_message = request.messages[-1]
    if last_message.role != "user":
        # å¦‚æœæœ€åä¸€æ¡ä¸æ˜¯ç”¨æˆ·æ¶ˆæ¯ï¼Œä½¿ç”¨å€’æ•°ç¬¬äºŒæ¡
        user_messages = [msg for msg in request.messages if msg.role == "user"]
        if not user_messages:
            # æ²¡æœ‰ç”¨æˆ·æ¶ˆæ¯ï¼Œè¿”å›é”™è¯¯
            from fastapi import HTTPException

            raise HTTPException(
                status_code=400,
                detail={
                    "error": {
                        "message": "No user message found in request",
                        "type": "invalid_request_error",
                        "code": "no_user_message",
                    }
                },
            )
        user_message = user_messages[-1].content
    else:
        user_message = last_message.content

    # ç”Ÿæˆ session_idï¼ˆå¯é€‰ï¼šä»è¯·æ±‚ä¸­æå–ï¼‰
    session_id = f"session-{uuid.uuid4().hex[:12]}"

    # æµå¼å“åº”
    if request.stream:
        return StreamingResponse(
            _stream_response(
                user_message=user_message,
                session_id=session_id,
                completion_id=completion_id,
                created_timestamp=created_timestamp,
                model=request.model,
            ),
            media_type="text/event-stream",
        )

    # éæµå¼å“åº”
    return await _non_stream_response(
        user_message=user_message,
        session_id=session_id,
        completion_id=completion_id,
        created_timestamp=created_timestamp,
        model=request.model,
    )


async def _non_stream_response(
    user_message: str,
    session_id: str,
    completion_id: str,
    created_timestamp: int,
    model: str,
) -> ChatCompletionResponse:
    """éæµå¼å“åº”"""
    # è°ƒç”¨ Agent
    app = get_agent_app()

    initial_state = {
        "messages": [HumanMessage(content=user_message)],
        "retrieved_docs": [],
        "tool_calls": [],
        "session_id": session_id,
        "next_step": None,
        "error": None,
        "confidence_score": None,
    }

    config = {"configurable": {"thread_id": session_id}}

    try:
        result = await app.ainvoke(initial_state, config)

        # æå– AI å“åº”
        ai_message = result["messages"][-1]
        if isinstance(ai_message, AIMessage):
            response_content = ai_message.content
        else:
            response_content = str(ai_message)

        # è®¡ç®— Token ä½¿ç”¨ï¼ˆç®€åŒ–ç‰ˆï¼Œä½¿ç”¨å­—ç¬¦æ•°ä¼°ç®—ï¼‰
        prompt_tokens = len(user_message) // 4
        completion_tokens = len(response_content) // 4
        total_tokens = prompt_tokens + completion_tokens

        # æ„å»º OpenAI æ ¼å¼å“åº”
        return ChatCompletionResponse(
            id=completion_id,
            created=created_timestamp,
            model=model,
            choices=[
                ChatCompletionChoice(
                    index=0,
                    message=ChatMessage(role="assistant", content=response_content),
                    finish_reason="stop",
                )
            ],
            usage=ChatCompletionUsage(
                prompt_tokens=prompt_tokens,
                completion_tokens=completion_tokens,
                total_tokens=total_tokens,
            ),
        )

    except Exception as e:
        logger.error(f"âŒ Agent execution failed: {e}")
        # è¿”å›é”™è¯¯å“åº”
        return ChatCompletionResponse(
            id=completion_id,
            created=created_timestamp,
            model=model,
            choices=[
                ChatCompletionChoice(
                    index=0,
                    message=ChatMessage(
                        role="assistant", content="æŠ±æ­‰ï¼Œç³»ç»Ÿé‡åˆ°äº†é—®é¢˜ï¼Œè¯·ç¨åå†è¯•ã€‚"
                    ),
                    finish_reason="stop",
                )
            ],
            usage=ChatCompletionUsage(
                prompt_tokens=0, completion_tokens=0, total_tokens=0
            ),
        )


async def _stream_response(
    user_message: str,
    session_id: str,
    completion_id: str,
    created_timestamp: int,
    model: str,
) -> AsyncGenerator[str, None]:
    """æµå¼å“åº”ï¼ˆSSEï¼‰"""
    app = get_agent_app()

    initial_state = {
        "messages": [HumanMessage(content=user_message)],
        "retrieved_docs": [],
        "tool_calls": [],
        "session_id": session_id,
        "next_step": None,
        "error": None,
        "confidence_score": None,
    }

    config = {"configurable": {"thread_id": session_id}}

    try:
        # å‘é€åˆå§‹ chunkï¼ˆroleï¼‰
        first_chunk = ChatCompletionChunk(
            id=completion_id,
            created=created_timestamp,
            model=model,
            choices=[
                ChatCompletionChunkChoice(
                    index=0,
                    delta=ChatCompletionChunkDelta(role="assistant"),
                    finish_reason=None,
                )
            ],
        )
        yield f"data: {first_chunk.model_dump_json()}\n\n"

        # æµå¼æ‰§è¡Œ Agent
        async for chunk in app.astream(initial_state, config):
            # æ£€æŸ¥æ˜¯å¦æœ‰æ–°çš„ AI æ¶ˆæ¯
            if "llm" in chunk:  # LLM èŠ‚ç‚¹çš„è¾“å‡º
                llm_output = chunk["llm"]
                messages = llm_output.get("messages", [])
                if messages:
                    ai_message = messages[-1]
                    if isinstance(ai_message, AIMessage):
                        content = ai_message.content

                        # å‘é€å†…å®¹ chunk
                        # æ³¨æ„ï¼šè¿™é‡Œå‘é€å®Œæ•´å†…å®¹ï¼Œå®é™…åº”è¯¥å‘é€å¢é‡
                        # ä¸ºäº†ç®€åŒ–ï¼Œæˆ‘ä»¬ä¸€æ¬¡æ€§å‘é€ï¼ˆLangGraph ä¸åŸç”Ÿæ”¯æŒ token-by-token æµå¼ï¼‰
                        content_chunk = ChatCompletionChunk(
                            id=completion_id,
                            created=created_timestamp,
                            model=model,
                            choices=[
                                ChatCompletionChunkChoice(
                                    index=0,
                                    delta=ChatCompletionChunkDelta(content=content),
                                    finish_reason=None,
                                )
                            ],
                        )
                        yield f"data: {content_chunk.model_dump_json()}\n\n"

        # å‘é€ç»“æŸ chunk
        final_chunk = ChatCompletionChunk(
            id=completion_id,
            created=created_timestamp,
            model=model,
            choices=[
                ChatCompletionChunkChoice(
                    index=0,
                    delta=ChatCompletionChunkDelta(),
                    finish_reason="stop",
                )
            ],
        )
        yield f"data: {final_chunk.model_dump_json()}\n\n"

        # å‘é€ [DONE]
        yield "data: [DONE]\n\n"

    except Exception as e:
        logger.error(f"âŒ Streaming failed: {e}")
        # å‘é€é”™è¯¯ chunk
        error_chunk = ChatCompletionChunk(
            id=completion_id,
            created=created_timestamp,
            model=model,
            choices=[
                ChatCompletionChunkChoice(
                    index=0,
                    delta=ChatCompletionChunkDelta(content="ç³»ç»Ÿé”™è¯¯ï¼Œè¯·ç¨åå†è¯•ã€‚"),
                    finish_reason="stop",
                )
            ],
        )
        yield f"data: {error_chunk.model_dump_json()}\n\n"
        yield "data: [DONE]\n\n"

