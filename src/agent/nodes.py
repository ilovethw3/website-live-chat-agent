"""
LangGraph Agent èŠ‚ç‚¹å‡½æ•°

å®šä¹‰ Agent å·¥ä½œæµä¸­çš„å„ä¸ªèŠ‚ç‚¹ï¼š
- router: è·¯ç”±å†³ç­–ï¼ˆæ˜¯å¦éœ€è¦æ£€ç´¢çŸ¥è¯†åº“ï¼‰
- retrieve: çŸ¥è¯†åº“æ£€ç´¢
- call_llm: è°ƒç”¨ LLM ç”Ÿæˆå“åº”
"""

import logging
from typing import Any

from langchain_core.messages import AIMessage, HumanMessage, SystemMessage

from src.agent.state import AgentState
from src.agent.tools import search_knowledge_for_agent
from src.core.config import settings
from src.services.llm_factory import create_llm

logger = logging.getLogger(__name__)


async def router_node(state: AgentState) -> dict[str, Any]:
    """
    è·¯ç”±èŠ‚ç‚¹ï¼šåˆ¤æ–­æ˜¯å¦éœ€è¦æ£€ç´¢çŸ¥è¯†åº“

    ç­–ç•¥ï¼š
    1. æ£€æŸ¥æ¶ˆæ¯ä¸­æ˜¯å¦åŒ…å«ç‰¹å®šå…³é”®è¯ï¼ˆäº§å“ã€æ”¿ç­–ã€ä»·æ ¼ç­‰ï¼‰
    2. æ£€æŸ¥é—®é¢˜ç±»å‹ï¼ˆç–‘é—®å¥ã€æŒ‡ä»¤ç­‰ï¼‰
    3. ç®€å•æ‰“æ‹›å‘¼ â†’ ç›´æ¥å›ç­”
    4. å¤æ‚é—®é¢˜ â†’ æ£€ç´¢çŸ¥è¯†åº“

    Args:
        state: å½“å‰ Agent çŠ¶æ€

    Returns:
        æ›´æ–°çš„çŠ¶æ€ï¼ˆåŒ…å« next_step å’Œ tool_callsï¼‰
    """
    # è·å–æœ€åä¸€æ¡ç”¨æˆ·æ¶ˆæ¯
    last_message = state["messages"][-1]
    if not isinstance(last_message, HumanMessage):
        # å¦‚æœä¸æ˜¯ç”¨æˆ·æ¶ˆæ¯ï¼Œç›´æ¥è·³è¿‡æ£€ç´¢
        return {"next_step": "direct", "tool_calls": []}

    query = last_message.content

    # å…³é”®è¯æ£€æµ‹ï¼ˆéœ€è¦æ£€ç´¢çš„å…³é”®è¯ï¼‰
    knowledge_keywords = [
        "äº§å“", "ä»·æ ¼", "æ”¿ç­–", "å¦‚ä½•", "ä»€ä¹ˆ", "å“ªé‡Œ", "æ€ä¹ˆ",
        "é€€è´§", "ä¿ä¿®", "å‘è´§", "é…é€", "æ”¯ä»˜", "è®¢å•",
        "åŠŸèƒ½", "å‚æ•°", "è§„æ ¼", "ä¼˜æƒ ", "æ´»åŠ¨"
    ]

    # ç®€å•æ‰“æ‹›å‘¼å…³é”®è¯ï¼ˆä¸éœ€è¦æ£€ç´¢ï¼‰
    greeting_keywords = ["ä½ å¥½", "æ‚¨å¥½", "hi", "hello", "æ—©ä¸Šå¥½", "æ™šä¸Šå¥½"]

    # åˆ¤æ–­æ˜¯å¦ä¸ºç®€å•æ‰“æ‹›å‘¼
    is_greeting = any(kw in query.lower() for kw in greeting_keywords)
    if is_greeting and len(query) < 20:  # çŸ­æ¶ˆæ¯ä¸”æ˜¯æ‰“æ‹›å‘¼
        logger.info("ğŸ¯ Router: Direct response (greeting)")
        return {
            "next_step": "direct",
            "tool_calls": [{"node": "router", "decision": "direct", "reason": "greeting"}]
        }

    # åˆ¤æ–­æ˜¯å¦éœ€è¦çŸ¥è¯†åº“æ£€ç´¢
    needs_retrieval = any(kw in query for kw in knowledge_keywords)

    if needs_retrieval:
        logger.info("ğŸ¯ Router: Retrieve from knowledge base (keywords matched)")
        return {
            "next_step": "retrieve",
            "tool_calls": [{"node": "router", "decision": "retrieve", "reason": "keywords"}]
        }
    else:
        logger.info("ğŸ¯ Router: Direct response (no keywords)")
        return {
            "next_step": "direct",
            "tool_calls": [{"node": "router", "decision": "direct", "reason": "no_keywords"}]
        }


async def retrieve_node(state: AgentState) -> dict[str, Any]:
    """
    çŸ¥è¯†åº“æ£€ç´¢èŠ‚ç‚¹

    ä» Milvus æ£€ç´¢ç›¸å…³çŸ¥è¯†åº“æ–‡æ¡£ã€‚

    Args:
        state: å½“å‰ Agent çŠ¶æ€

    Returns:
        æ›´æ–°çš„çŠ¶æ€ï¼ˆåŒ…å« retrieved_docs å’Œ confidence_scoreï¼‰
    """
    # è·å–æŸ¥è¯¢
    if not state["messages"]:
        logger.warning("âš ï¸ Retrieve node: empty messages")
        return {"retrieved_docs": [], "confidence_score": 0.0}

    last_message = state["messages"][-1]
    query = last_message.content if isinstance(last_message, HumanMessage) else ""

    if not query:
        logger.warning("âš ï¸ Retrieve node: empty query")
        return {"retrieved_docs": [], "confidence_score": 0.0}

    # æ‰§è¡Œæ£€ç´¢
    results = await search_knowledge_for_agent(query, top_k=settings.rag_top_k)

    if not results:
        logger.info(f"ğŸ“­ Retrieve node: no results found for '{query}'")
        return {"retrieved_docs": [], "confidence_score": 0.0}

    # æ ¼å¼åŒ–æ£€ç´¢ç»“æœ
    formatted_docs = []
    for i, result in enumerate(results, 1):
        metadata = result.get("metadata", {})
        title = metadata.get("title", "æœªå‘½åæ–‡æ¡£")
        url = metadata.get("url", "")

        doc_text = f"[æ–‡æ¡£{i}] {title}"
        if url:
            doc_text += f" (æ¥æº: {url})"
        doc_text += f"\n{result['text']}"

        formatted_docs.append(doc_text)

    # è®¡ç®—ç½®ä¿¡åº¦ï¼ˆä½¿ç”¨æœ€é«˜åˆ†æ•°ï¼‰
    confidence = results[0]["score"] if results else 0.0

    logger.info(
        f"âœ… Retrieve node: found {len(results)} documents, "
        f"confidence={confidence:.2f}"
    )

    return {
        "retrieved_docs": formatted_docs,
        "confidence_score": confidence,
        "tool_calls": state.get("tool_calls", []) + [
            {
                "node": "retrieve",
                "results_count": len(results),
                "top_score": confidence
            }
        ]
    }


async def call_llm_node(state: AgentState) -> dict[str, Any]:
    """
    LLM ç”ŸæˆèŠ‚ç‚¹

    è°ƒç”¨ LLM ç”Ÿæˆæœ€ç»ˆå“åº”ã€‚

    ç­–ç•¥ï¼š
    - å¦‚æœæœ‰æ£€ç´¢åˆ°çš„æ–‡æ¡£ï¼ˆretrieved_docsï¼‰ï¼Œä½¿ç”¨ RAG æ¨¡å¼
    - å¦‚æœæ²¡æœ‰æ–‡æ¡£ï¼Œç›´æ¥å¯¹è¯æ¨¡å¼

    Args:
        state: å½“å‰ Agent çŠ¶æ€

    Returns:
        æ›´æ–°çš„çŠ¶æ€ï¼ˆåŒ…å«æ–°çš„ AI æ¶ˆæ¯ï¼‰
    """
    retrieved_docs = state.get("retrieved_docs", [])

    # æ„å»ºç³»ç»Ÿæç¤ºè¯
    if retrieved_docs:
        # RAG æ¨¡å¼
        # retrieved_docs æ˜¯å­—å…¸åˆ—è¡¨ï¼Œéœ€è¦æå– text å­—æ®µ
        context = "\n\n".join([doc.get("text", str(doc)) for doc in retrieved_docs])
        system_prompt = f"""ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„ç½‘ç«™å®¢æœåŠ©æ‰‹ã€‚

**çŸ¥è¯†åº“ä¸Šä¸‹æ–‡**:
{context}

**å›ç­”è¦æ±‚**:
1. **ä¼˜å…ˆä½¿ç”¨çŸ¥è¯†åº“ä¿¡æ¯**å›ç­”é—®é¢˜
2. å¼•ç”¨çŸ¥è¯†åº“æ—¶ï¼Œè¯´æ˜æ¥æºï¼ˆå¦‚ï¼š"æ ¹æ®æˆ‘ä»¬çš„é€€è´§æ”¿ç­–..."ï¼‰
3. å¦‚æœçŸ¥è¯†åº“ä¿¡æ¯ä¸è¶³ä»¥å›ç­”é—®é¢˜ï¼ŒåŸºäºå¸¸è¯†ç¤¼è²Œå›ç­”
4. **ä¸ç¡®å®šæ—¶ï¼Œè¯šå®å‘ŠçŸ¥**ï¼ˆå¦‚ï¼š"æŠ±æ­‰ï¼Œæˆ‘åœ¨çŸ¥è¯†åº“ä¸­æœªæ‰¾åˆ°ç›¸å…³ä¿¡æ¯"ï¼‰
5. ä¿æŒä¸“ä¸šã€å‹å¥½çš„è¯­æ°”

**ç¦æ­¢**:
- ä¸è¦ç¼–é€ çŸ¥è¯†åº“ä¸­ä¸å­˜åœ¨çš„ä¿¡æ¯
- ä¸è¦ç»™å‡ºä¸çŸ¥è¯†åº“çŸ›ç›¾çš„ç­”æ¡ˆ
"""
    else:
        # ç›´æ¥å¯¹è¯æ¨¡å¼
        system_prompt = """ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šã€å‹å¥½çš„ç½‘ç«™å®¢æœåŠ©æ‰‹ã€‚

**å›ç­”è¦æ±‚**:
1. ä¿æŒç¤¼è²Œã€ä¸“ä¸šçš„è¯­æ°”
2. ç®€æ´æ˜äº†åœ°å›ç­”é—®é¢˜
3. å¦‚æœé—®é¢˜æ¶‰åŠå…·ä½“çš„äº§å“ã€æ”¿ç­–ç­‰ä¿¡æ¯ï¼Œå»ºè®®ç”¨æˆ·æŸ¥çœ‹å®˜ç½‘æˆ–è”ç³»äººå·¥å®¢æœ
4. ä¸è¦ç¼–é€ å…·ä½“çš„äº§å“ä¿¡æ¯æˆ–æ”¿ç­–ç»†èŠ‚
"""

    # æ„å»ºæ¶ˆæ¯åˆ—è¡¨
    messages = [
        SystemMessage(content=system_prompt),
        *state["messages"]
    ]

    # è°ƒç”¨ LLM
    try:
        llm = create_llm()
        response = await llm.ainvoke(messages)

        logger.info(f"ğŸ¤– LLM response generated (mode: {'RAG' if retrieved_docs else 'direct'})")

        return {
            "messages": [response],
            "tool_calls": state.get("tool_calls", []) + [
                {
                    "node": "call_llm",
                    "mode": "RAG" if retrieved_docs else "direct",
                    "response_length": len(response.content) if hasattr(response, 'content') else 0
                }
            ]
        }

    except Exception as e:
        logger.error(f"âŒ LLM call failed: {e}")

        # è¿”å›é”™è¯¯æ¶ˆæ¯
        error_message = AIMessage(
            content="æŠ±æ­‰ï¼Œç³»ç»Ÿé‡åˆ°äº†ä¸€äº›é—®é¢˜ï¼Œè¯·ç¨åå†è¯•ã€‚"
        )

        return {
            "messages": [error_message],
            "error": str(e),
            "tool_calls": state.get("tool_calls", []) + [
                {"node": "call_llm", "error": str(e)}
            ]
        }

